# Environment variables for local development
# Copy this file to .dev.vars and fill in your values

# OpenAI API Key for client authentication
# This key is required for clients to access the proxy endpoints
OPENAI_API_KEY=sk-your-openai-api-key-here

# ChatGPT API Configuration
CHATGPT_LOCAL_CLIENT_ID=your_client_id_here
CHATGPT_RESPONSES_URL=https://chatgpt.com/backend-api/codex/responses

# Authentication - paste your auth.json content here (as a JSON string)
# You can get this by running the Codex login and copying the auth.json content
OPENAI_CODEX_AUTH={"OPENAI_API_KEY": null, "tokens": {"id_token": "...", "access_token": "...", "refresh_token": "...", "account_id": "..."}, "last_refresh": "..."}

# Ollama API endpoint (for local Ollama instance)
OLLAMA_API_URL=http://localhost:11434

# Optional debugging and reasoning settings
DEBUG_MODEL=
REASONING_EFFORT=medium  # Options: minimal, low, medium, high
REASONING_SUMMARY=auto   # Options: auto, concise, detailed, none (on=concise, off=none)
# Options: tagged, openai, o3, r1, hidden, all
# When set to "all", the worker exposes multiple prefixed endpoints with per-endpoint
# compatibility:
#   /tagged/v1/*   -> tagged
#   /r1/v1/*       -> r1
#   /o3/v1/*       -> o3
#   /openai/v1/* -> openai
#   /hidden/v1/*   -> hidden
# The root /v1/* continues to work and defaults to openai under ALL mode.
REASONING_COMPAT=openai
# Alias supported as well (if set):
# REASONING_OUTPUT_MODE=openai
VERBOSE=false            # Options: true, false

# ------------------
# Instructions source (optional)
# ------------------
# Defaults to ChatMock prompt:
#   INSTRUCTIONS_BASE_URL=https://raw.githubusercontent.com/RayBytes/ChatMock/main/prompt.md
# Codex-specific defaults to ChatMock's codex prompt when not set.
# Offline fallback is embedded.
# INSTRUCTIONS_BASE_URL=
# INSTRUCTIONS_CODEX_URL=

# ------------------
# Upstream provider overrides (optional)
# ------------------

# 1) Choose your upstream endpoint
# Option A: Full responses URL (highest priority)
# UPSTREAM_RESPONSES_URL=https://example.com/v1/responses

# Option B: Base URL + wire path (defaults to /responses)
# UPSTREAM_BASE_URL=https://example.com/v1
# UPSTREAM_WIRE_API_PATH=/responses

# 2) Choose how to authenticate to the upstream
#    chatgpt_token     -> use OAuth tokens from OPENAI_CODEX_AUTH (default)
#    apikey_auth_json  -> read key from OPENAI_CODEX_AUTH[UPSTREAM_AUTH_ENV_KEY] (default key: OPENAI_API_KEY)
#    apikey_env        -> use UPSTREAM_API_KEY below
# UPSTREAM_AUTH_MODE=chatgpt_token

# When using apikey_auth_json, set the key name in OPENAI_CODEX_AUTH to read (default: OPENAI_API_KEY)
# UPSTREAM_AUTH_ENV_KEY=OPENAI_API_KEY

# When using apikey_env, place the upstream API key here
# UPSTREAM_API_KEY=sk-your-upstream-api-key

# Optional: customize header/scheme (defaults: Authorization / Bearer)
# UPSTREAM_AUTH_HEADER=Authorization
# UPSTREAM_AUTH_SCHEME=Bearer

# Optional: tools schema compatibility (default: nested)
# nested -> tools: [{ type: 'function', function: { name, ... } }]
# flat   -> tools: [{ type: 'function', name, ... }]
# UPSTREAM_TOOLS_FORMAT=nested

# ------------------
# Client header forwarding (optional)
# ------------------
# Controls forwarding of client headers to upstream to preserve client characteristics
# off             -> do not forward (default)
# safe            -> forward UA, Accept-Language, sec-ch-*, X-Forwarded-For, etc.
# list            -> forward only headers listed below (comma-separated); values come from client request
# override        -> set final header values from JSON map below
# override-codex  -> override only 'user-agent' and 'originator' using Codex's construction logic.
#                    - originator: defaults to 'codex_cli_rs' (override via FORWARD_CLIENT_HEADERS_CODEX_ORIGINATOR)
#                    - User-Agent: 'originator/<version> (<osType> <osVersion>; <arch>) <terminal>'
#                      version is fetched from GitHub latest release (openai/codex rust tag); override via
#                      FORWARD_CLIENT_HEADERS_CODEX_VERSION. Terminal derived from environment like VSCode.
#                      In Workers runtime missing values fall back to 'unknown'.
# FORWARD_CLIENT_HEADERS_MODE=off
# FORWARD_CLIENT_HEADERS_LIST=User-Agent,Accept-Language
# FORWARD_CLIENT_HEADERS_OVERRIDE={"User-Agent":"MyApp/1.2.3","Accept":"text/event-stream"}
# FORWARD_CLIENT_HEADERS_OVERRIDE_CODEX={"user-agent":"codex_cli_rs/0.36.0 (Windows 10.0.26100; x86_64) vscode/1.104.0","originator":"codex_cli_rs"}

# Optional Codex builder settings
# FORWARD_CLIENT_HEADERS_CODEX_VERSION=0.36.0
# FORWARD_CLIENT_HEADERS_CODEX_ORIGINATOR=codex_cli_rs
# TERM_PROGRAM=vscode
# TERM_PROGRAM_VERSION=1.104.0
# FORWARD_CLIENT_HEADERS_CODEX_OS_TYPE=Windows
# FORWARD_CLIENT_HEADERS_CODEX_OS_VERSION=10.0.26100
# FORWARD_CLIENT_HEADERS_CODEX_ARCH=x86_64

# ------------------
# Models exposure (optional)
# ------------------
# Comma-separated or JSON array of model IDs listed by /v1/models
# Examples:
# EXPOSE_MODELS="gpt-5,gpt-5-codex,codex-mini-latest"
# EXPOSE_MODELS='["gpt-5","gpt-5-codex","codex-mini-latest"]'
EXPOSE_MODELS="gpt-5,gpt-5-latest,gpt-5-codex,gpt-5-codex-latest,codex-mini-latest"


# ----------------------------------------------------------------------------------
# Auto-generation for User-Agent environment variables
# ----------------------------------------------------------------------------------
# To automatically populate the environment variables required for a detailed User-Agent
# (like OS, architecture, and editor version), run the following command in your terminal.
#
# For best results, run this from the integrated terminal within VS Code, as this allows
# the script to detect TERM_PROGRAM and TERM_PROGRAM_VERSION.
#
# Command:
# npm run gen:env
#
# This will print a list of key-value pairs. Copy them into your .dev.vars file.
# Example generated variables:
#
# TERM_PROGRAM="vscode"
# TERM_PROGRAM_VERSION="1.91.1"
# FORWARD_CLIENT_HEADERS_CODEX_OS_TYPE="Windows"
# FORWARD_CLIENT_HEADERS_CODEX_OS_VERSION="10.0.22631"
# FORWARD_CLIENT_HEADERS_CODEX_ARCH="x86_64"
#
