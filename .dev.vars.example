# ------------------------------------------------------------------------------
# .dev.vars.example â€” Sample environment for local development
# Copy to ".dev.vars" and edit values. Lines starting with "#" are ignored.
# This file is read by scripts/prepare-wrangler-config.mjs during `npm run dev`
# and by Docker dev runtime (wrangler dev). Values become Worker bindings via
# a generated .wrangler.generated.toml.
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# 1) Client access control (required for non-public endpoints)
# - Your clients must send: Authorization: Bearer <OPENAI_API_KEY>
# - If this is empty or missing, auth is skipped (all endpoints open).
# ------------------------------------------------------------------------------
OPENAI_API_KEY=dev-local-access-key

# ------------------------------------------------------------------------------
# 2) Upstream provider selection & auth (pick ONE strategy)
# Default: ChatGPT Responses endpoint with OAuth tokens in OPENAI_CODEX_AUTH.
#    - UPSTREAM_AUTH_MODE:
#        chatgpt_token     -> use OAuth tokens in OPENAI_CODEX_AUTH (default)
#        apikey_env        -> send UPSTREAM_API_KEY
#        apikey_auth_json  -> send OPENAI_CODEX_AUTH[UPSTREAM_AUTH_ENV_KEY] (default key: OPENAI_API_KEY)
# ------------------------------------------------------------------------------
# Base ChatGPT Responses URL (used unless UPSTREAM_RESPONSES_URL or UPSTREAM_BASE_URL is set)
CHATGPT_RESPONSES_URL=https://chatgpt.com/backend-api/codex/responses
# Client ID used in token refresh flow (kept for compatibility)
CHATGPT_LOCAL_CLIENT_ID=app_EMoamEEZ73f0CkXaXp7hrann

# OAuth tokens and optional API key stored as a single-line JSON
# Fill these values after performing your auth flow.
OPENAI_CODEX_AUTH={"OPENAI_API_KEY":null,"tokens":{"id_token":"...","access_token":"...","refresh_token":"...","account_id":"..."},"last_refresh":"..."}

# Strategy A (default): use ChatGPT OAuth tokens from OPENAI_CODEX_AUTH
UPSTREAM_AUTH_MODE=chatgpt_token

# Strategy B: use a direct upstream API key from env
#UPSTREAM_AUTH_MODE=apikey_env
#UPSTREAM_API_KEY=sk-your-upstream-api-key
#UPSTREAM_AUTH_HEADER=Authorization
#UPSTREAM_AUTH_SCHEME=Bearer

# Strategy C: use key embedded inside OPENAI_CODEX_AUTH JSON
#UPSTREAM_AUTH_MODE=apikey_auth_json
#UPSTREAM_AUTH_ENV_KEY=OPENAI_API_KEY

# Optional: Override upstream URL selection
# Highest priority:
#UPSTREAM_RESPONSES_URL=https://api.example.com/v1/responses
# Or: base + (optional) wire path (default "/responses"):
#UPSTREAM_BASE_URL=https://api.example.com/v1
#UPSTREAM_WIRE_API_PATH=/responses

# Tools wire-format compatibility for upstream (recommended: nested for OpenAI)
UPSTREAM_TOOLS_FORMAT=nested

# ------------------------------------------------------------------------------
# 3) Reasoning controls
# ------------------------------------------------------------------------------
# Effort: minimal | low | medium | high
REASONING_EFFORT=low
# Summary: auto | concise | detailed | none  (aliases: on=concise, off=none)
REASONING_SUMMARY=auto
# Output mode: openai (default) | tagged | o3 | r1 | hidden | all
# When set to "all", the worker additionally exposes:
#   /tagged/v1/*, /r1/v1/*, /o3/v1/*, /openai/v1/*, /hidden/v1/*
REASONING_OUTPUT_MODE=openai

# ------------------------------------------------------------------------------
# 4) Instructions (prompt) source
# Provide custom instruction URLs or leave empty to use built-in defaults.
# ------------------------------------------------------------------------------
#INSTRUCTIONS_BASE_URL=
#INSTRUCTIONS_CODEX_URL=
#INSTRUCTIONS_SANITIZE_PATCH=true
#INSTRUCTIONS_SANITIZE_LEVEL=auto  # auto | basic | strict | off

# ------------------------------------------------------------------------------
# 5) Ollama integration (optional)
# Enables pass-through routes under /api/* (e.g., POST /api/chat, GET /api/tags)
# ------------------------------------------------------------------------------
# Local default:
OLLAMA_API_URL=http://localhost:11434

# ------------------------------------------------------------------------------
# 6) Models exposed by GET /v1/models
# Accepts CSV or JSON array. Defaults mirror OpenAI-compatible IDs used here.
# ------------------------------------------------------------------------------
EXPOSE_MODELS="gpt-5,gpt-5-latest,gpt-5-codex,gpt-5-codex-latest,codex-mini-latest"

# ------------------------------------------------------------------------------
# 7) Client header forwarding (advanced)
# Controls how selected client headers are forwarded upstream.
#   off      -> do not forward (default)
#   safe     -> forward allowlist (User-Agent, Accept-Language, sec-ch-*, X-Forwarded-*)
#   list     -> forward only headers listed in FORWARD_CLIENT_HEADERS_LIST
#   override -> explicitly set final headers from JSON map (Authorization never overridden)
#   override-codex / override_codex -> derive 'User-Agent' and 'originator' like codex CLI
# ------------------------------------------------------------------------------
FORWARD_CLIENT_HEADERS_MODE=off
# When mode=list, provide comma-separated header names (case-insensitive)
#FORWARD_CLIENT_HEADERS_LIST=User-Agent,Accept-Language
# When mode=override, provide JSON map of header overrides
#FORWARD_CLIENT_HEADERS_OVERRIDE={"User-Agent":"MyApp/1.2.3","Accept":"text/event-stream"}
# When mode=override-codex, you may override generated 'user-agent' and 'originator'
#FORWARD_CLIENT_HEADERS_OVERRIDE_CODEX={"user-agent":"codex_cli_rs/0.36.0 (Windows 10.0.26100; x86_64) vscode/1.104.0","originator":"codex_cli_rs"}
# Optionally pin version/originator for codex UA generation
#FORWARD_CLIENT_HEADERS_CODEX_VERSION=0.36.0
#CODEX_INTERNAL_ORIGINATOR_OVERRIDE=codex_cli_rs
#FORWARD_CLIENT_HEADERS_CODEX_ORIGINATOR=codex_cli_rs

# Optional OS/arch/editor hints for codex UA (used when client hints unavailable)
#FORWARD_CLIENT_HEADERS_CODEX_OS_TYPE=Windows
#FORWARD_CLIENT_HEADERS_CODEX_OS_VERSION=10.0.26100
#FORWARD_CLIENT_HEADERS_CODEX_ARCH=x86_64
#FORWARD_CLIENT_HEADERS_CODEX_EDITOR=vscode/1.104.0

# Optional terminal detection (helps build UA if editor not detected)
#TERM_PROGRAM=vscode
#TERM_PROGRAM_VERSION=1.104.0
#WEZTERM_VERSION=
#KONSOLE_VERSION=
#VTE_VERSION=
#WT_SESSION=
#KITTY_WINDOW_ID=
#ALACRITTY_SOCKET=
#TERM=xterm-256color

# ------------------------------------------------------------------------------
# 8) Logging / debugging
# ------------------------------------------------------------------------------
# Minimal request logs in routes (string "true" or "false")
VERBOSE=false
# Force a specific model id regardless of request (useful for testing)
#DEBUG_MODEL=gpt-5-codex

# ------------------------------------------------------------------------------
# 9) Cloudflare KV binding (optional)
# If set, the generator injects a kv_namespaces block with this ID bound as "KV".
# ------------------------------------------------------------------------------
#KV_ID=

# ------------------------------------------------------------------------------
# 10) Dev/runtime toggles for generator (Docker/local only)
# - GENERATE_WRANGLER_VARS:
#     1/true  -> Add [vars] to generated toml (values visible in wrangler logs)
#     0/false -> Do not add [vars] (default). Prefer this to keep logs clean.
# - WRANGLER_EMIT_DEV_VARS:
#     1/true  -> Emit a .dev.vars snapshot from current env if [vars] is not generated
#     0/false -> Do not emit snapshot
# The docker-compose dev service sets GENERATE_WRANGLER_VARS=0 and WRANGLER_EMIT_DEV_VARS=1.
# ------------------------------------------------------------------------------
#GENERATE_WRANGLER_VARS=0
#WRANGLER_EMIT_DEV_VARS=1

# ------------------------------------------------------------------------------
# 11) Node runtime only (not used by Workers dev)
# The Node server listens on PORT when running `npm run start:node`.
# ------------------------------------------------------------------------------
#PORT=8787
