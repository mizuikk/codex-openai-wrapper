# Environment variables for local development
# Copy this file to .dev.vars and fill in your values

# OpenAI API Key for client authentication
# This key is required for clients to access the proxy endpoints
OPENAI_API_KEY=sk-your-openai-api-key-here

# ChatGPT API Configuration
CHATGPT_LOCAL_CLIENT_ID=your_client_id_here
CHATGPT_RESPONSES_URL=https://chatgpt.com/backend-api/codex/responses

# Authentication - paste your auth.json content here (as a JSON string)
# You can get this by running the Codex login and copying the auth.json content
OPENAI_CODEX_AUTH={"OPENAI_API_KEY": null, "tokens": {"id_token": "...", "access_token": "...", "refresh_token": "...", "account_id": "..."}, "last_refresh": "..."}

# Ollama API endpoint (for local Ollama instance)
OLLAMA_API_URL=http://localhost:11434

# Optional debugging and reasoning settings
DEBUG_MODEL=
REASONING_EFFORT=medium  # Options: minimal, low, medium, high
REASONING_SUMMARY=auto   # Options: auto, concise, detailed, none (on=concise, off=none)
REASONING_COMPAT=think-tags  # Options: think-tags, standard, o3, r1, legacy, current, hide
VERBOSE=false            # Options: true, false

# ------------------
# Instructions source (optional)
# ------------------
# Defaults to ChatMock prompt:
#   INSTRUCTIONS_BASE_URL=https://raw.githubusercontent.com/RayBytes/ChatMock/main/prompt.md
# Codex-specific defaults to ChatMock's codex prompt when not set.
# Offline fallback is embedded.
# INSTRUCTIONS_BASE_URL=
# INSTRUCTIONS_CODEX_URL=

# ------------------
# Upstream provider overrides (optional)
# ------------------

# 1) Choose your upstream endpoint
# Option A: Full responses URL (highest priority)
# UPSTREAM_RESPONSES_URL=https://example.com/v1/responses

# Option B: Base URL + wire path (defaults to /responses)
# UPSTREAM_BASE_URL=https://example.com/v1
# UPSTREAM_WIRE_API_PATH=/responses

# 2) Choose how to authenticate to the upstream
#    chatgpt_token     -> use OAuth tokens from OPENAI_CODEX_AUTH (default)
#    apikey_auth_json  -> read key from OPENAI_CODEX_AUTH[UPSTREAM_AUTH_ENV_KEY] (default key: OPENAI_API_KEY)
#    apikey_env        -> use UPSTREAM_API_KEY below
# UPSTREAM_AUTH_MODE=chatgpt_token

# When using apikey_auth_json, set the key name in OPENAI_CODEX_AUTH to read (default: OPENAI_API_KEY)
# UPSTREAM_AUTH_ENV_KEY=OPENAI_API_KEY

# When using apikey_env, place the upstream API key here
# UPSTREAM_API_KEY=sk-your-upstream-api-key

# Optional: customize header/scheme (defaults: Authorization / Bearer)
# UPSTREAM_AUTH_HEADER=Authorization
# UPSTREAM_AUTH_SCHEME=Bearer

# Optional: tools schema compatibility (default: nested)
# nested -> tools: [{ type: 'function', function: { name, ... } }]
# flat   -> tools: [{ type: 'function', name, ... }]
# UPSTREAM_TOOLS_FORMAT=nested

# ------------------
# Client header forwarding (optional)
# ------------------
# Controls forwarding of client headers to upstream to preserve client characteristics
# off       -> do not forward (default)
# safe      -> forward UA, Accept-Language, sec-ch-*, X-Forwarded-For, etc.
# list      -> forward only headers listed below (comma-separated); values come from client request
# override  -> set final header values from JSON map below
# FORWARD_CLIENT_HEADERS_MODE=off
# FORWARD_CLIENT_HEADERS_LIST=User-Agent,Accept-Language
# FORWARD_CLIENT_HEADERS_OVERRIDE={"User-Agent":"MyApp/1.2.3","Accept":"text/event-stream"}

# ------------------
# Models exposure (optional)
# ------------------
# Comma-separated or JSON array of model IDs listed by /v1/models
# Examples:
# EXPOSE_MODELS="gpt-5,gpt-5-codex,codex-mini-latest"
# EXPOSE_MODELS='["gpt-5","gpt-5-codex","codex-mini-latest"]'
EXPOSE_MODELS="gpt-5,gpt-5-latest,gpt-5-codex,gpt-5-codex-latest,codex-mini-latest"
